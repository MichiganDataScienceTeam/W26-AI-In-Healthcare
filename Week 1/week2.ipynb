{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNVBh96+OLj+O7K0tLMfHnA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Data Visualizations\n","\n","### **All exercises will be using your chosen dataset**"],"metadata":{"id":"4hEReReHkR_w"}},{"cell_type":"markdown","source":["## Step 1: Import your dataset using the tutorial from the slides"],"metadata":{"id":"WAEriF75kYgn"}},{"cell_type":"markdown","source":["## Step 2: Install necessary libraries. New libraries are Matplotlib and Seaborn for week 2. We will also use scikit-learn for Principal Component Analysis."],"metadata":{"id":"qhO90HzYkaJs"}},{"cell_type":"code","source":["!pip install pandas numpy matplotlib seaborn scikit-learn\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA"],"metadata":{"id":"chqsPphulBv5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Exercises\n","\n","### Documentation for Reference\n","\n","#### [Pandas](https://pandas.pydata.org/docs/)\n","\n","#### [NumPy](https://numpy.org/devdocs/)\n","\n","#### [Matplotlib](https://matplotlib.org/stable/)\n","\n","#### [Seaborn](https://seaborn.pydata.org/)\n","\n","#### [Scikit-Learn PCA](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)\n"],"metadata":{"id":"ZPNOrLQ7lhrS"}},{"cell_type":"markdown","source":["In Machine Learning, the target **y** is the variable that the model is trying to predict given a set of features **X**. In regression tasks, **y** is a numerical value and in classification, it is a categorical value. Identify the target variable that you are using and state what type it is."],"metadata":{"id":"x5fgdAg9Mgx6"}},{"cell_type":"markdown","source":["### Exercise 1: Correlations"],"metadata":{"id":"hMAAq6_qEWTy"}},{"cell_type":"code","source":["# TODO: Read dataset into a Pandas dataframe"],"metadata":{"id":"5UBmONmTEohr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Create a correlation matrix heatmap (you first need to use pd.corr()). Use sns for visualizing the heatmap)\n","# Keep in mind that these correlations highlight linear relationships between variables"],"metadata":{"id":"zkZ109O9Euwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Plot a histogram of the correlations of all features related to the desired target variable.\n","# Use matrix[\"target\"] and matplotlib's plt.hist() for the histogram"],"metadata":{"id":"0vZY_HcJGmUl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Which feature(s) are highly correlated to the target?"],"metadata":{"id":"Xwn8OUaMHRXn"}},{"cell_type":"markdown","source":["### Exercise 2: Numerical Feature Visualizations"],"metadata":{"id":"7eKz1dUfEaBO"}},{"cell_type":"code","source":["# TODO: Identify one numerical feature to work with and plot a distribution of this feature using plt.hist() or sns.histplot().\n","# Also, use plt.title() to name this graph"],"metadata":{"id":"BE6cbbGlIv83"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Plot an ECDF graph using seaborn - sns.ecdfplot(). Ths stands for Empirical Cumulative Distribution function."],"metadata":{"id":"RYQLRTaLMPjq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How is the feature distributed? A **uniform distribution** means that the **histogram** is level and the **ECDF** is a straight line. If possible, identify anything that might hint at bias in the sampling of this data such as abnormally high or abnormally low values that are present in the distribution."],"metadata":{"id":"NUMm7OCvKTUD"}},{"cell_type":"markdown","source":["### Exercise 3: Categorical Feature Visualizations"],"metadata":{"id":"aO-sZFpiEaHt"}},{"cell_type":"code","source":["# TODO: Identify one categorical feature to work with and plot a distribution of it using sns.countplot().\n","# If you don't have aa categorical variable, you can use pd.cut() to create one"],"metadata":{"id":"qW58IVXKLcQy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Same as before, determine how the categories are distributed and highlight any biases that seem present."],"metadata":{"id":"CYHuAvz5OCGh"}},{"cell_type":"markdown","source":["### Exercise 4: Target vs. Numerical Feature\n"],"metadata":{"id":"Uq92tSqkEaKN"}},{"cell_type":"markdown","source":["#### Use the previous numerical feature and the chosen target to create the following visualizations."],"metadata":{"id":"hjRD6y_UXYDE"}},{"cell_type":"markdown","source":["**Do these exercises if your target is numerical**"],"metadata":{"id":"FJr-epmbQTzY"}},{"cell_type":"code","source":["# TODO: Create a seaborn or matplotlib scatterplot plot (scatterplot()) which shows a graphical relationship between these two variables."],"metadata":{"id":"TodEVORRQNjJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Create a matplotlib scatter plot (pyplot.scatter()). Use plt.xlabel and plt.ylabel"],"metadata":{"id":"qWHfqEatQRGS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Create a seaborn lmplot() which creates a regression line for the the scattered data"],"metadata":{"id":"dzc-4nAdU5se"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write down insights into how these variables are related (or not)."],"metadata":{"id":"gzlWb5TTV1ct"}},{"cell_type":"markdown","source":["**Do these exercises if your target is categorical**"],"metadata":{"id":"nYdUsE94Qaew"}},{"cell_type":"code","source":["# TODO: Create a seaborn boxplot() for the two variables"],"metadata":{"id":"PhaNR4DZQctH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Create a seaborn violinplot() which shows the distribution of the numerical data"],"metadata":{"id":"OWwLKbXHQd4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write down insights into how these variables are related (or not)."],"metadata":{"id":"nx7ApHeaWmOm"}},{"cell_type":"markdown","source":["### Exercise 5: Target vs. Categorical Feature"],"metadata":{"id":"rKkXE6SKEaMY"}},{"cell_type":"markdown","source":["#### Use the previous categorical feature  and the chosen target to create the following visualizations."],"metadata":{"id":"QSzZXKmTXe78"}},{"cell_type":"markdown","source":["**Do these exercises if your target is numerical**"],"metadata":{"id":"vfQ0qpwVWsPD"}},{"cell_type":"code","source":["# TODO: Create a seaborn boxplot() for these two variables"],"metadata":{"id":"AbeTHQt4W4IQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Create a seaborn violinplot(). Use inner=\"quartile\" to view the percentiles of the distribution"],"metadata":{"id":"nGbf7PFAW36j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write down insights into how these variables are related (or not)."],"metadata":{"id":"_9mYmxRmW24G"}},{"cell_type":"markdown","source":["**Do these exercises if your target is categorical**"],"metadata":{"id":"OD8X_l8XWylw"}},{"cell_type":"code","source":["# TODO: Use pd.crosstab() which creates a frequency table of the categorical variables"],"metadata":{"id":"OUYjg1krW5-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Using the crosstab above, create a seaborn heatmap() which visualizes this relationship"],"metadata":{"id":"ByMvj4_zW53B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Write down insights into how these variables are related (or not)."],"metadata":{"id":"S3ct1Z0nW3Tb"}},{"cell_type":"markdown","source":["### Exercise 6: Principal Component Analysis"],"metadata":{"id":"_vkuQWn6EaOF"}},{"cell_type":"markdown","source":["Principal Component Analysis is a powerful tool mainly used for dimensionality reduction by transforming the data into principal components (uncorrelated features) which are combinations of the original features in the data. In our case, we will be using it to create a 2d visualization of the data we are using."],"metadata":{"id":"VhaQIeTWaeVh"}},{"cell_type":"code","source":["# TODO: Use pandas to drop the target variable and put this into a variable X_data.\n","X_data = ..."],"metadata":{"id":"2yf8_1cCbyt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Before doing PCA, we need to encode the categorical variables. One way of doing this is one-hot encoding which turns the variables into numerical 1's and 0's for the algorithm to understand."],"metadata":{"id":"UF8c7Ht4cWqM"}},{"cell_type":"code","source":["# TODO: get_dummies is one-hot encoding the categorical variables\n","# Also, put the target varible into the y_data variable\n","\n","X_data = pd.get_dummies(X_data, drop_first = True)\n","y_data = ..."],"metadata":{"id":"0uJOuWclbDyQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: This makes sure all features contribute equally for the analysis\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X_data)"],"metadata":{"id":"B9fVFb3Vc5-7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: The first line initializes the pca algorithm.\n","# the second line creates the principal components\n","\n","pca = PCA()\n","X_pca = pca.fit_transform(X_scaled)"],"metadata":{"id":"Ak0qM30hfFAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can determine how many PCs we need based on a variance explanation threshold. We do this by plotting a cumulative density plot of explained variance."],"metadata":{"id":"fzrlZ-9ChvzZ"}},{"cell_type":"code","source":["# TODO: Use plt.plot() to plot 'np.cumsum(pca.explained_variance_ratio_)'"],"metadata":{"id":"W1E1irTHf_yU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The y-axis is the variance (the spread) of the original data. How many principal components are needed to explain ~90% of the variance in your data?"],"metadata":{"id":"ohDHeA8BhDaR"}},{"cell_type":"code","source":["# TODO: These lines are for graphing first two principal components\n","\n","pca = PCA(n_components = 2)\n","X_pca = pca.fit_transform(X_scaled)"],"metadata":{"id":"NAPfR20xmnj4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: This code creates a scatterplot of the first two principal components using sns.scatterplot().\n","\n","df_pca = pd.DataFrame(X_pca)\n","df_pca[\"target\"] = y_data\n","sns.scatterplot(data=df_pca, x=0, y=1, hue='target')"],"metadata":{"id":"rgK8RSKshr3K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The x-axis is the first PC and the y-axis is the second PC. If your target was categorical, Identify if the clusters between the categories are noticeable. If your target was numerical, determine if the color gradient varies smoothly along the principal components. Note that points that are close together have similar data profiles."],"metadata":{"id":"W2GdR88DoLhA"}},{"cell_type":"markdown","source":["This last line of code gives us the features that were involved in the first principal component. Note that the first principal component is designed to capture the maximum possible variance."],"metadata":{"id":"dnDKOkZjnoN_"}},{"cell_type":"code","source":["# Define the loadings for each feature on each Principal Component (the correlations of each feature in PC)\n","loadings = pd.DataFrame(\n","    pca.components_.T,\n","    index=X_data.columns,\n","    columns=[f\"PC{i+1}\" for i in range(pca.n_components_)]\n",")\n","\n","# Show the top-contributing features for PC1\n","loadings[\"PC1\"].sort_values(ascending=False).head(10)"],"metadata":{"id":"j8NzNpB7pDMo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the highest correlated feature in PC1?"],"metadata":{"id":"UK6pCXlzpvHK"}},{"cell_type":"markdown","source":["### Exercise 7: Further Visualizations"],"metadata":{"id":"hKVoj1vlEafF"}},{"cell_type":"markdown","source":["#### Continue creating data visualizations for other other features and further understand your data! Utilize the documentation."],"metadata":{"id":"E4YsDxw_Tj7k"}}]}